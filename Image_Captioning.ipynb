{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patelkanak23/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485e6f78",
      "metadata": {
        "id": "485e6f78"
      },
      "source": [
        "## ***Importing Required Libraries***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fba30f",
      "metadata": {
        "id": "09fba30f"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f2be6e",
      "metadata": {
        "id": "83f2be6e"
      },
      "source": [
        "## ***Extracting Features from images***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d42285e",
      "metadata": {
        "id": "8d42285e"
      },
      "outputs": [],
      "source": [
        "def extract_features(directory):\n",
        "\n",
        "\n",
        "    features = dict()\n",
        "    for name in listdir(directory):\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        image_id = name.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "        print('>%s' % name)\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d23987",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0d23987",
        "outputId": "cd572e58-5965-41c7-83fa-6e88d1002bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 4s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134260544 (512.16 MB)\n",
            "Trainable params: 134260544 (512.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = VGG16()\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b02ed9e",
      "metadata": {
        "collapsed": true,
        "id": "9b02ed9e",
        "outputId": "4888b1cb-bdb4-4a62-ce39-663bf712c138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'listdir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-40a3e0c24a08>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"C:\\ML-DATA\\Flickr8k_Dataset\\Flicker8k_Dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracted Features: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-df12d05644a5>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'listdir' is not defined"
          ]
        }
      ],
      "source": [
        "directory = r\"C:\\ML-DATA\\Flickr8k_Dataset\\Flicker8k_Dataset\"\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "\n",
        "dump(features, open('features.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a03ada92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a03ada92",
        "outputId": "c4a5f46c-7cb7-4c0c-a2e7-7d90f59e6f67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from pickle import load\n",
        "all_features = load(open('features.pkl', 'rb'))\n",
        "len(list(all_features.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ad8961",
      "metadata": {
        "id": "b5ad8961"
      },
      "source": [
        "## ***Preparing Text Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64974eb7",
      "metadata": {
        "id": "64974eb7"
      },
      "outputs": [],
      "source": [
        "def load(file):\n",
        " file = open(filename, 'r')\n",
        " text = file.read()\n",
        " file.close()\n",
        " return text\n",
        "\n",
        "filename = \"Flickr8k.token.txt\"\n",
        "doc = load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736f9837",
      "metadata": {
        "id": "736f9837"
      },
      "outputs": [],
      "source": [
        "def create_desc(description):\n",
        "    captions={}\n",
        "    for line in doc.split('\\n'):\n",
        "        if len(line)<2:\n",
        "            continue\n",
        "        letters=line.split()\n",
        "        img_id=letters[0]\n",
        "        desc=letters[1:]\n",
        "        img_id = img_id.split('.')[0]\n",
        "        desc = ' '.join(desc)\n",
        "        if img_id not in captions:\n",
        "            captions[img_id]=[]\n",
        "        captions[img_id].append(desc)\n",
        "    return captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5517ac97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5517ac97",
        "outputId": "fe32482e-4093-4d3d-8067-af3f817cf7ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8092"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "descriptions=create_desc(doc)\n",
        "len(descriptions.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96177f3b",
      "metadata": {
        "id": "96177f3b"
      },
      "outputs": [],
      "source": [
        "def clean_text(desc):\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, text in desc.items():\n",
        "        for i in range(len(text)):\n",
        "            text1 = text[i].split()\n",
        "            text1 = [word.lower() for word in text1]\n",
        "\n",
        "\n",
        "            text1 = [word.translate(table) for word in text1]\n",
        "\n",
        "\n",
        "            text1 = [word for word in text1 if len(word) > 1]\n",
        "            text1 = [word for word in text1 if word.isalpha()]\n",
        "            text[i] = ' '.join(text1)\n",
        "clean_text(descriptions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df737d66",
      "metadata": {
        "id": "df737d66"
      },
      "outputs": [],
      "source": [
        "def to_vocabulary(descriptions):\n",
        "    all_desc = set()\n",
        "    for key in descriptions.keys():\n",
        "        for d in descriptions[key]:\n",
        "            all_desc.update(d.split())\n",
        "    return all_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81cbaca4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81cbaca4",
        "outputId": "585432cd-a54e-44d9-a0e8-41a9372e3830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 8763\n"
          ]
        }
      ],
      "source": [
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8079d335",
      "metadata": {
        "id": "8079d335"
      },
      "outputs": [],
      "source": [
        "def save(descriptions, file_path):\n",
        "    saved = []\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            saved.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(saved)\n",
        "\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b23628",
      "metadata": {
        "id": "51b23628"
      },
      "outputs": [],
      "source": [
        "save(descriptions, \"C:\\\\ML-DATA\\\\Image_Captioning\\\\descriptions.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba4750f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bba4750f",
        "outputId": "5ab9d7cd-6819-4c48-8fcd-a48ffd7817aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000268201_693b08cb0e child in pink dress is climbing up set of stairs in an entry way\\n1000268201_693b08cb0e girl going into wooden building\\n1000268201_693b08cb0e little girl climbing into wooden play'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "sample =open(\"C:\\ML-DATA\\Image_Captioning\\descriptions.txt\", 'r')\n",
        "sample.read(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5ed2a4",
      "metadata": {
        "id": "aa5ed2a4"
      },
      "source": [
        "## ***Load Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b911887b",
      "metadata": {
        "id": "b911887b"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        "\n",
        " file = open(filename, 'r')\n",
        " text = file.read()\n",
        " file.close()\n",
        " return text\n",
        "\n",
        "\n",
        "def load_set(filename):\n",
        " doc = load_doc(filename)\n",
        " dataset = list()\n",
        "\n",
        " for line in doc.split('\\n'):\n",
        "\n",
        "  if len(line) < 1:\n",
        "   continue\n",
        "\n",
        "  identifier = line.split('.')[0]\n",
        "  dataset.append(identifier)\n",
        " return set(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b355d5",
      "metadata": {
        "id": "80b355d5"
      },
      "outputs": [],
      "source": [
        "def load_clean_descriptions(filename, dataset):\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "\n",
        "        if image_id in dataset:\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "\n",
        "    return descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600fb052",
      "metadata": {
        "id": "600fb052"
      },
      "outputs": [],
      "source": [
        "def load_photo_features(filename, dataset):\n",
        "    try:\n",
        "        with open(filename, 'rb') as f:\n",
        "            all_features = pickle.load(f)\n",
        "        features = {}\n",
        "        for k in dataset:\n",
        "            if k in all_features:\n",
        "                features[k] = all_features[k]\n",
        "            else:\n",
        "                print(f\"Warning: Key '{k}' not found in features file\")\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading features: {str(e)}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78c536d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e78c536d",
        "outputId": "ea6a7304-1e5e-4966-c694-13793eaa4b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n"
          ]
        }
      ],
      "source": [
        "filename = \"Flickr_8k.trainImages.txt\"\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions(r\"C:\\ML-DATA\\Image_Captioning\\descriptions.txt\", train)\n",
        "# descriptions\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "\n",
        "# photo features\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "if train_features is not None:\n",
        "    print('Photos: train=%d' % len(train_features))\n",
        "else:\n",
        "    print(\"Failed to load photo features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0a6e54",
      "metadata": {
        "id": "fc0a6e54"
      },
      "source": [
        "## ***Encoding Text***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d678f69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d678f69",
        "outputId": "a4d461f5-61dd-4cdb-a4ee-c8d5e95e85d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 7579\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def to_lines(descriptions):\n",
        " all_desc = list()\n",
        " for key in descriptions.keys():\n",
        "  [all_desc.append(d) for d in descriptions[key]]\n",
        " return all_desc\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    lines=to_lines(descriptions)\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdd9040",
      "metadata": {
        "id": "3fdd9040"
      },
      "source": [
        "## ***Creating Input-Output pairs***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859591af",
      "metadata": {
        "id": "859591af"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    for desc in desc_list:\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            X1.append(photo)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c869c2b",
      "metadata": {
        "id": "8c869c2b"
      },
      "outputs": [],
      "source": [
        "def max_length(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d7cfd2",
      "metadata": {
        "id": "27d7cfd2"
      },
      "source": [
        "## ***Defining Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbcdd155",
      "metadata": {
        "id": "bbcdd155"
      },
      "outputs": [],
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc808fae",
      "metadata": {
        "id": "fc808fae"
      },
      "outputs": [],
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "    while True:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            photo = photos[key][0]\n",
        "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "            yield (np.array(in_img), np.array(in_seq)), np.array(out_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452f91da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "452f91da",
        "outputId": "679d20dd-cf7a-4bcb-950a-35e4bffa14d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ]
        }
      ],
      "source": [
        "filename = \"Flickr_8k.trainImages.txt\"\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions(r\"C:\\ML-DATA\\Image_Captioning\\descriptions.txt\", train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb098a87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb098a87",
        "outputId": "a9b80bf8-a40d-4c93-c8c7-2f3109e86b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(47, 4096)\n",
            "(47, 34)\n",
            "(47, 7579)\n"
          ]
        }
      ],
      "source": [
        "generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "inputs, outputs = next(generator)\n",
        "print(inputs[0].shape)\n",
        "print(inputs[1].shape)\n",
        "print(outputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da85dba2",
      "metadata": {
        "id": "da85dba2"
      },
      "source": [
        "## ***Training Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8064786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8064786",
        "outputId": "f460446e-42a5-4c64-9da8-bcab965c6066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 34)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 34, 256)              1940224   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 4096)                 0         ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 34, 256)              0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
            "                                                                     'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 7579)                 1947803   ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5527963 (21.09 MB)\n",
            "Trainable params: 5527963 (21.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "6000/6000 [==============================] - 536s 89ms/step - loss: 4.6737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000/6000 [==============================] - 533s 89ms/step - loss: 3.9013\n",
            "6000/6000 [==============================] - 533s 89ms/step - loss: 3.6520\n",
            "6000/6000 [==============================] - 531s 89ms/step - loss: 3.5192\n"
          ]
        }
      ],
      "source": [
        "model = define_model(vocab_size, max_length)\n",
        "\n",
        "epochs = 4\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    model.save('model_' + str(i) + '.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89297e8a",
      "metadata": {
        "id": "89297e8a"
      },
      "outputs": [],
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QyWiLB_bE4hG",
      "metadata": {
        "id": "QyWiLB_bE4hG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "    actual, predicted = [],[]\n",
        "    for key, desc_list in descriptions.items():\n",
        "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "        references = [d.split() for d in desc_list]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "\n",
        "    print('BLEU-1:', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2:',corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3:',corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4:',corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yR1ruiTjFBGq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR1ruiTjFBGq",
        "outputId": "75819dea-0d5e-4687-d92a-f76fc028e10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "BLEU-1: 0.5448866498740554\n",
            "BLEU-2: 0.29916856376955797\n",
            "BLEU-3: 0.20154242123012905\n",
            "BLEU-4: 0.0916960212061892\n"
          ]
        }
      ],
      "source": [
        "filename = 'Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "\n",
        "test_descriptions = load_clean_descriptions(r\"C:\\ML-DATA\\Image_Captioning\\descriptions.txt\", test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ItDqNOWGFwhq",
      "metadata": {
        "id": "ItDqNOWGFwhq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}